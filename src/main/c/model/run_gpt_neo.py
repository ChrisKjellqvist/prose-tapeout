# Load model directly
import torch
from transformers import GPT2Tokenizer
from gptneo_clone import GPTNeoForCausalLM
import os
from torch.profiler.profiler import *
from torch.profiler import *
from run_bert_tiny import save_tensor
import run_bert_tiny as rbt
import sys
import transformers

args = sys.argv

do_inf = "-do_inf" in args
do_profile = "-do_profile" in args
save_model = "-save_model" in args
max_layer_save = 1


if __name__ == "__main__":
    model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-125M")
    tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-125m")

    # Text taken without permission from https://www.math.columbia.edu/~woit/wordpress/?p=13924
    # Interesting (bearish) article about some new AI stuff for string theory
    inputSeq = ["Looking at these new neural network calculations, what’s remarkable is that they’re essentially "
                "a return to a failed project of nearly 40 years ago. "
                "In 1985 the exciting new idea was that maybe compactifying a 10d superstring on a Calabi-Yau "
                "would give the Standard Model. "
                "It quickly became clear that this wasn’t going to work. "
                "A minor problem was that there were quite a few classes of Calabi-Yaus, but the really big "
                "problem was that the Calabi-Yaus in each class were parametrized by a large dimensional moduli "
                "space. One needed some method of “moduli stabilization” that would pick out specific moduli "
                "parameters. Without that, the moduli parameters became massless fields, introducing a huge host"
                " of unobserved new long-range interactions. The state of the art 20 years later is that endless "
                "arguments rage over whether Rube Goldberg-like constructions such as KKLT can consistently "
                "stabilize moduli (if they do, you get the “landscape” and can’t calculate anything anyway, since "
                "these constructions give exponentially large numbers of possibilities). "
                "To conclude, the reason why these new neural network calculations are problematic is that they "]
    inputSeq = inputSeq * 4
    input_ids = tokenizer(inputSeq, return_tensors="pt").input_ids
    print("tensor shape is ", input_ids.shape)
    print("batch size is ", input_ids.shape[0])
    print(model)
    print("model parameter data type is " + str(next(model.parameters()).dtype))

    if do_inf:
        if do_profile:
            log_num = 0
            prev_logs = [int(a.split("_")[-1]) for a in list(os.listdir("log"))]
            if len(prev_logs) > 0:
                log_num = max(prev_logs) + 1

            with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True,
                         with_flops=True, with_modules=True,
                         on_trace_ready=torch.profiler.tensorboard_trace_handler(f'./log/gptneo_{log_num}'),
                         use_cuda=False) as prof:  # if you want a tensorboard log
                with record_function("model_inference"):
                    output = model.generate(input_ids,
                                            do_sample=True,
                                            max_length=512,
                                            pad_token_id=tokenizer.eos_token_id,
                                            use_cache=True)
        else:
            output = model.generate(input_ids,
                                    do_sample=True,
                                    max_length=512,
                                    pad_token_id=tokenizer.eos_token_id,
                                    use_cache=True)

        full_answer = tokenizer.decode(output[0], skip_special_tokens=True)
        # get the part that the AI generated by removing the input sequence
        generated_answer = full_answer[len(inputSeq[0]):]
        print(generated_answer)
    if save_model:
        dirname = "gpt_neo"
        os.makedirs(dirname, exist_ok=True)
        layer_dict = {}
        print("Config: ", model.config)
        print("Num layers: ", len(model.transformer.h))
        for i, layer in enumerate(model.transformer.h):
            if i == max_layer_save:
                break
            layer_name = f"transformer.h.{i}"
            save_tensor(layer.ln_1.weight, f"{dirname}/{layer_name}_ln_1_weight.npy")
            save_tensor(layer.ln_1.bias, f"{dirname}/{layer_name}_ln_1_bias.npy")
            save_tensor(layer.attn.attention.k_proj.weight, f"{dirname}/{layer_name}_attn_kproj_weight.npy")
            save_tensor(layer.attn.attention.v_proj.weight, f"{dirname}/{layer_name}_attn_vproj_weight.npy")
            save_tensor(layer.attn.attention.q_proj.weight, f"{dirname}/{layer_name}_attn_qproj_weight.npy")
            save_tensor(layer.attn.attention.out_proj.weight, f"{dirname}/{layer_name}_attn_outproj_weight.npy")
            save_tensor(layer.attn.attention.out_proj.bias, f"{dirname}/{layer_name}_attn_outproj_bias.npy")
            save_tensor(layer.ln_2.weight, f"{dirname}/{layer_name}_ln_2_weight.npy")
            save_tensor(layer.ln_2.bias, f"{dirname}/{layer_name}_ln_2_bias.npy")
            save_tensor(layer.mlp.c_fc.weight, f"{dirname}/{layer_name}_mlp_cfc_weight.npy")
            save_tensor(layer.mlp.c_fc.bias, f"{dirname}/{layer_name}_mlp_cfc_bias.npy")
            save_tensor(layer.mlp.c_proj.weight, f"{dirname}/{layer_name}_mlp_cproj_weight.npy")
            save_tensor(layer.mlp.c_proj.bias, f"{dirname}/{layer_name}_mlp_cproj_bias.npy")
            layer_dict.update({
                f"{layer_name}_ln_1_weight": layer.ln_1.weight.shape,
                f"{layer_name}_ln_1_bias": layer.ln_1.bias.shape,
                f"{layer_name}_attn_kproj_weight": layer.attn.attention.k_proj.weight.shape,
                f"{layer_name}_attn_vproj_weight": layer.attn.attention.v_proj.weight.shape,
                f"{layer_name}_attn_qproj_weight": layer.attn.attention.q_proj.weight.shape,
                f"{layer_name}_attn_outproj_weight": layer.attn.attention.out_proj.weight.shape,
                f"{layer_name}_attn_outproj_bias": layer.attn.attention.out_proj.bias.shape,
                f"{layer_name}_ln_2_weight": layer.ln_2.weight.shape,
                f"{layer_name}_ln_2_bias": layer.ln_2.bias.shape,
                f"{layer_name}_mlp_cfc_weight": layer.mlp.c_fc.weight.shape,
                f"{layer_name}_mlp_cfc_bias": layer.mlp.c_fc.bias.shape,
                f"{layer_name}_mlp_cproj_weight": layer.mlp.c_proj.weight.shape,
                f"{layer_name}_mlp_cproj_bias": layer.mlp.c_proj.bias.shape,
            })
        if i is -1:
            save_tensor(model.transformer.ln_f.weight, f"{dirname}/transformer_ln_f_weight.npy")
            save_tensor(model.transformer.ln_f.bias, f"{dirname}/transformer_ln_f_bias.npy")
            save_tensor(model.lm_head.weight, f"{dirname}/lm_head_weight.npy")
        layer_dict.update({
            "transformer_ln_f_weight": model.transformer.ln_f.weight.shape,
            "transformer_ln_f_bias": model.transformer.ln_f.bias.shape,
            "lm_head_weight": model.lm_head.weight.shape,
        })
        with open(f"{dirname}/layer_dict.txt", "w") as f:
            for key, value in layer_dict.items():
                # split the value into a series of space separated numbers
                k = value
                if isinstance(k, dict):
                    k = " ".join(str(x) for x in k.values())
                elif isinstance(k, tuple):
                    k = " ".join(str(x) for x in k)
                elif isinstance(k, torch.Size):
                    k = " ".join(str(x) for x in k)
                else:
                    k = " ".join(str(x) for x in k)
                f.write(f"{key} {k}\n")
        print("Saved model to ", dirname)
        print(f"Estimated model size is {(rbt.total_tensor_size_bytes / 1024 / 1024):.02f} MB if we were to convert to BF16")
