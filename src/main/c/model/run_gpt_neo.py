# Load model directly
import torch
from transformers import GPT2Tokenizer
from gptneo_clone import GPTNeoForCausalLM
import os
from torch.profiler.profiler import *
from torch.profiler import *

from pyutils import save_tensor
import pyutils

do_inf = True
do_profile = False
save_model = True
layers_to_emit = 2

if __name__ == "__main__":
    dirname = "gpt_neo"
    model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-125M")
    tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-125M")
    if save_model:
        os.system(f"rm -rf {dirname} && mkdir {dirname}")
        pyutils.enable_saving()
        if layers_to_emit != -1:
            pyutils.set_layer_save_limit(layers_to_emit)

    # Text taken without permission from https://www.math.columbia.edu/~woit/wordpress/?p=13924
    # Interesting (bearish) article about some new AI stuff for string theory
    
    # 5 tokens
    # inputSeq = ["What's my name?"]
    
    # 8 tokens
    inputSeq = ["Duke University is a place in the"]
    input_ids = tokenizer(inputSeq, return_tensors="pt").input_ids
    print("tensor shape is ", input_ids.shape)
    print("batch size is ", input_ids.shape[0])
    print(model)
    print("model parameter data type is " + str(next(model.parameters()).dtype))

    if do_inf:
        if do_profile:
            log_num = 0
            prev_logs = [int(a.split("_")[-1]) for a in list(os.listdir("log"))]
            if len(prev_logs) > 0:
                log_num = max(prev_logs) + 1

            with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True,
                         with_flops=True, with_modules=True,
                         on_trace_ready=torch.profiler.tensorboard_trace_handler(f'./log/gptneo_{log_num}'),
                         use_cuda=False) as prof:  # if you want a tensorboard log
                with record_function("model_inference"):
                    output = model.generate(input_ids,
                                            do_sample=True,
                                            max_length=512,
                                            pad_token_id=tokenizer.eos_token_id,
                                            use_cache=True)
        else:
            output = model.generate(input_ids,
                                    do_sample=True,
                                    max_length=9,
                                    pad_token_id=tokenizer.eos_token_id,
                                    use_cache=True)

        full_answer = tokenizer.decode(output[0], skip_special_tokens=True)
        # get the part that the AI generated by removing the input sequence
        generated_answer = full_answer[len(inputSeq[0]):]
        print(generated_answer)
    print("Config: ", model.config)
    print("Num layers: ", len(model.transformer.h))
    for i, layer in enumerate(model.transformer.h):
        layer_name = f"transformer.h.{i}"
        save_tensor(layer.ln_1.weight, dirname, f"{layer_name}_ln_1_weight", i)
        save_tensor(layer.ln_1.bias, dirname, f"{layer_name}_ln_1_bias", i)
        save_tensor(layer.attn.attention.k_proj.weight, dirname, f"{layer_name}_attn_kproj_weight", i)
        save_tensor(layer.attn.attention.v_proj.weight, dirname, f"{layer_name}_attn_vproj_weight", i)
        save_tensor(layer.attn.attention.q_proj.weight, dirname, f"{layer_name}_attn_qproj_weight", i)
        save_tensor(layer.attn.attention.out_proj.weight, dirname, f"{layer_name}_attn_outproj_weight", i)
        save_tensor(layer.attn.attention.out_proj.bias, dirname, f"{layer_name}_attn_outproj_bias", i)
        save_tensor(layer.ln_2.weight, dirname, f"{layer_name}_ln_2_weight", i)
        save_tensor(layer.ln_2.bias, dirname, f"{layer_name}_ln_2_bias", i)
        save_tensor(layer.mlp.c_fc.weight, dirname, f"{layer_name}_mlp_cfc_weight", i)
        save_tensor(layer.mlp.c_fc.bias, dirname, f"{layer_name}_mlp_cfc_bias", i)
        save_tensor(layer.mlp.c_proj.weight, dirname, f"{layer_name}_mlp_cproj_weight", i)
        save_tensor(layer.mlp.c_proj.bias, dirname, f"{layer_name}_mlp_cproj_bias", i)
    save_tensor(model.transformer.ln_f.weight, dirname, f"transformer_ln_f_weight", i)
    save_tensor(model.transformer.ln_f.bias, dirname, f"transformer_ln_f_bias", i)
    save_tensor(model.lm_head.weight, dirname, f"lm_head_weight", i)
    pyutils.write_layer_dict(f"{dirname}/layer_dict.txt")
